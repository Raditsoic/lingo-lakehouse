{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "your 131072x1 screen size is bogus. expect trouble\n",
      "24/12/03 08:49:51 WARN Utils: Your hostname, cloud resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)\n",
      "24/12/03 08:49:51 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/home/soic/.local/lib/python3.12/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/soic/.ivy2/cache\n",
      "The jars for the packages stored in: /home/soic/.ivy2/jars\n",
      "org.apache.hadoop#hadoop-aws added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-529f503d-2f12-4985-b60b-55035b68bbdd;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.hadoop#hadoop-aws;3.2.0 in central\n",
      "\tfound com.amazonaws#aws-java-sdk-bundle;1.11.375 in central\n",
      ":: resolution report :: resolve 259ms :: artifacts dl 7ms\n",
      "\t:: modules in use:\n",
      "\tcom.amazonaws#aws-java-sdk-bundle;1.11.375 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-aws;3.2.0 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   2   |   0   |   0   |   0   ||   2   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-529f503d-2f12-4985-b60b-55035b68bbdd\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 2 already retrieved (0kB/7ms)\n",
      "24/12/03 08:49:52 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/12/03 08:49:53 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "24/12/03 08:49:56 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+-----------------+-----------+--------------------+--------------------+-------+--------------+------------+---------------+------------+---------------+\n",
      "|user_id| timestamp|learning_language|ui_language|           lexeme_id|       lexeme_string|  delta|      p_recall|history_seen|history_correct|session_seen|session_correct|\n",
      "+-------+----------+-----------------+-----------+--------------------+--------------------+-------+--------------+------------+---------------+------------+---------------+\n",
      "| u:fowg|1362160674|               en|         es|ca886bc339ea78c58...|orange/orange<n><sg>|    841|           1.0|           4|              3|           2|              2|\n",
      "| u:duK3|1362160678|               en|         es|a94a938b71c9bd335...|which/which<det><...| 257458|0.666666666667|           5|              4|           3|              2|\n",
      "| u:duK3|1362160678|               en|         es|7dab41a78fec1de7e...|     hats/hat<n><pl>| 861434|           0.5|           3|              3|           2|              1|\n",
      "| u:duK3|1362160678|               en|         es|9ac50d1a5c3247453...|green/green<adj><...| 603942|           1.0|           3|              3|           1|              1|\n",
      "| u:duK3|1362160678|               en|         es|4adf5cd40d521b02a...|have/have<vblex><...| 256054|           1.0|          14|             12|           1|              1|\n",
      "| u:duK3|1362160678|               en|         es|764e3dc3b4ba72565...|     hello/hello<ij>|7605794|           1.0|           1|              1|           1|              1|\n",
      "| u:duK3|1362160678|               en|         es|64fb4435d0bd27297...|you/prpers<prn><o...| 604353|           0.0|           4|              3|           1|              0|\n",
      "| u:duK3|1362160678|               en|         es|1fc2f0656f2fe60d0...|   do/do<vbdo><pres>| 256054|           1.0|           7|              7|           2|              2|\n",
      "| u:duK3|1362160678|               en|         es|4f5b0cd71bfa77c08...|which/which<prn><...| 257458|           1.0|           5|              5|           1|              1|\n",
      "| u:duK3|1362160678|               en|         es|a617ed646a251e339...| are/be<vbser><pres>|8210981|0.666666666667|           4|              4|           3|              2|\n",
      "| u:duK3|1362160678|               en|         es|529445ad4ea7fd181...|  reply/reply<n><sg>| 257458|           1.0|           1|              1|           1|              1|\n",
      "| u:duK3|1362160678|               en|         es|da166f3d66b929683...|who/who<prn><itg>...| 258284|           1.0|           5|              5|           1|              1|\n",
      "| u:duK3|1362160678|               en|         es|8c8952fbf398f9964...|        red/red<adj>| 603942|           0.5|           4|              4|           2|              1|\n",
      "| u:duK3|1362160678|               en|         es|1c1d2e454e5e8b32f...|question/question...| 257458|           1.0|           2|              2|           1|              1|\n",
      "| u:duK3|1362160678|               en|         es|aaf58e27968fec878...| when/when<adv><itg>| 257458|           1.0|           2|              1|           1|              1|\n",
      "| u:duK3|1362160678|               en|         es|487ce8262508986af...|whose/whose<prn><...| 257458|           1.0|           4|              4|           1|              1|\n",
      "| u:duK3|1362160678|               en|         es|7dd6b30fc37ca7348...|    bear/bear<n><sg>|6396498|           1.0|           6|              6|           1|              1|\n",
      "| u:duK3|1362160678|               en|         es|a6f949fa049279ad5...|where/where<adv><...| 258284|           1.0|           4|              4|           2|              2|\n",
      "| u:duK3|1362160678|               en|         es|dfc953d62306359e4...|   shoes/shoe<n><pl>|1210464|           1.0|           2|              2|           1|              1|\n",
      "| u:duK3|1362160678|               en|         es|ef1f94c0e977e97bd...|chicken/chicken<n...|6396498|           1.0|           2|              2|           1|              1|\n",
      "+-------+----------+-----------------+-----------+--------------------+--------------------+-------+--------------+------------+---------------+------------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/03 08:50:09 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"MinIO-Spark-ELT\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:3.2.0\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", \"minio\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", \"minio123\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://localhost:9000\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.aws.credentials.provider\", \"org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "\n",
    "input_path = \"s3a://duolingo/raw/duolingo_batch_1362160679.parquet\"\n",
    "df = spark.read.parquet(input_path)\n",
    "\n",
    "# transformed_df = df.withColumn(\"new_column\", df[\"history_correct\"] + df[\"session_correct\"])\n",
    "df.show()\n",
    "\n",
    "ml_output_path = \"s3a://duolingo-ml/duolingo_transformed.parquet\"\n",
    "visual_output_path = \"s3a://duolingo-visual/duolingo_transformed.parquet\"\n",
    "# transformed_df.write.mode(\"overwrite\").parquet(ml_output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_df = df.drop(['ui_language', ])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
